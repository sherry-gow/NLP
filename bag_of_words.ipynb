{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words Model\n",
    "\n",
    "A simplifying representation of text used in natural language processing and information retrieval. A text (sentence or a document) is represented as the bag of its words, disregarding grammar and word order. The multiplicity of the words are being kept. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pros and Cons of Bag of words model\n",
    "- Pros\n",
    "    - If the dataset is small and context is domain specific, using BoW model may work better than word embedding, because it's hard to find corresponding vector from pre-trained word embedding models\n",
    "    - It's relative easy to build a baseline model. For example using scikit-learn library, just a few lines of code the model can be built. I will provide some examples below. \n",
    "    \n",
    "- Cons\n",
    "    - Large feature dimension considering each token is a feature\n",
    "    - Sparcity: a dataset comprises mostly zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction vs. Feature Selection\n",
    "```Feature Extraction```: Transforming arbitary data, such as text or images into ```numerical``` features usable for machine learning\n",
    "\n",
    "\n",
    "```Feature Selection```: A machine learning technique applied on these features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction most intivitive ways:\n",
    "    1. Assign a fixed integer id to each word occuring in any document of the training set (build a dictionary from words to integer indices)\n",
    "    2. For each document #i, count the number of occurrences of each word w, and store it in x[i, j] as the value of feature #j where j is the index of word w in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### high-dimensional ```sparse``` dataset\n",
    "\n",
    "- A sparse dataset is a dataset that is comprised of mostly zero values\n",
    "- Sparse matrices are distinct from matrices with mostly non-zero values, which are referred to as dense matrices *\n",
    "- Sparse Matrices can cause problmes regards to space and time complexity\n",
    "- Sparsity can be computed as below:\n",
    "    - $sparsity = 1.0 - count_nonzero(A) / A.size$\n",
    "\n",
    "### Examples for Sparse matrics for specific machine learning problems:\n",
    "- Whether or not a user has watched a movie in a movie catalog\n",
    "- Count of the number of words in a document corpus\n",
    "- Whether or not a user has purchased a product in a product list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input array: \n",
      "[[1 0 0 1 0 0]\n",
      " [0 0 2 0 0 1]\n",
      " [0 0 0 2 0 0]]\n",
      "\n",
      "\n",
      "Converted to Sparse Matrix: \n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 2)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 3)\t2\n",
      "\n",
      "\n",
      "Convert back to Dense Matrix: \n",
      "[[1 0 0 1 0 0]\n",
      " [0 0 2 0 0 1]\n",
      " [0 0 0 2 0 0]]\n",
      "\n",
      "\n",
      "\n",
      "The array size is: 18\n",
      "The input array sparsity: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "# dense to sparse\n",
    "from numpy import array\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy import count_nonzero\n",
    "\n",
    "# create dense matrix\n",
    "A = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]])\n",
    "print('''The input array: \n",
    "{}\n",
    "'''.format(A))\n",
    "print()\n",
    "# convert to sparse matrix (CSR method)\n",
    "S = csr_matrix(A)\n",
    "print('''Converted to Sparse Matrix: \n",
    "{}\n",
    "'''.format(S))\n",
    "print()\n",
    "# reconstruct dense matrix\n",
    "B = S.todense()\n",
    "print('''Convert back to Dense Matrix: \n",
    "{}\n",
    "'''.format(B))\n",
    "print()\n",
    "\n",
    "sparsity = 1.0 - count_nonzero(A) / A.size\n",
    "print('''\n",
    "The array size is: {}\n",
    "The input array sparsity: {}'''.format(A.size, sparsity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction -  Word Counting\n",
    "- ```tokenization```: strings and int id for each possible token. Potentially remove the stop words (the most common words in a language)\n",
    "- ```counting```: the occurrences of tokens in each document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is the source text:\n",
      "['SciPy provides tools for creating sparse matrices using multiple data structures', 'A dense matrix stored in a NumPy array can be converted into a sparse matrix']\n",
      "\n",
      "Below is the vocabulary based on the text above:\n",
      "{'scipy': 15, 'provides': 14, 'tools': 19, 'for': 7, 'creating': 4, 'sparse': 16, 'matrices': 10, 'using': 20, 'multiple': 12, 'data': 5, 'structures': 18, 'dense': 6, 'matrix': 11, 'stored': 17, 'in': 8, 'numpy': 13, 'array': 0, 'can': 2, 'be': 1, 'converted': 3, 'into': 9}\n",
      "\n",
      "Total vocabularies (CountVectorizer removes the stopwords): 21 \n",
      "\n",
      "token(word) 'sparse' has index: 16 \n",
      "\n",
      "index 11 has token (word): 'matrix' \n",
      "\n",
      "Let's count the the tokens\n",
      "After transform() to count the tokens, the word count feature has the following shape: \n",
      "(2, 21)\n",
      "\n",
      "The word count feature type is: \n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "Word count feature presented as sparse matrix:\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t2\n",
      "  (1, 13)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "\n",
      "Convert word count feature from sparse matrix to dense matrix(2 by 21):\n",
      "[[0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1]\n",
      " [1 1 1 1 0 0 1 0 1 1 0 2 0 1 0 0 1 1 0 0 0]]\n",
      "The input array sparsity: 0.47619047619047616\n",
      "\n",
      "The stop words list in skit learn library:\n",
      "frozenset({'eight', 'out', 'still', 'we', 'name', 'was', 'how', 'myself', 'for', 'could', 'they', 'please', 'about', 'ie', 'find', 'inc', 'then', 'both', 'nobody', 'whereupon', 'since', 'put', 're', 'so', 'mine', 'ten', 'top', 'as', 'any', 'amount', 'anyhow', 'after', 'back', 'somehow', 'you', 'eg', 'whatever', 'whereby', 'though', 'why', 'several', 'been', 'fire', 'fifty', 'six', 'sometimes', 'by', 'thru', 'get', 'hence', 'where', 'hereafter', 'yourself', 'a', 'three', 'everyone', 'done', 'often', 'towards', 'whose', 'made', 'may', 'call', 'never', 'side', 'thereafter', 'cant', 'hasnt', 'thin', 'their', 'even', 'bill', 'beside', 'latter', 'might', 'seems', 'noone', 'none', 'be', 'anywhere', 'further', 'interest', 'now', 'am', 'ours', 'show', 'take', 'due', 'her', 'toward', 'someone', 'fifteen', 'yourselves', 'that', 'my', 'last', 'nothing', 'to', 'not', 'what', 'thereby', 'beforehand', 'every', 'own', 'becoming', 'there', 'upon', 'one', 'becomes', 'this', 'forty', 'each', 'all', 'meanwhile', 'system', 'thence', 'whether', 'more', 'else', 'serious', 'perhaps', 'me', 'together', 'which', 'four', 'full', 'per', 'himself', 'indeed', 'nor', 'de', 'give', 'moreover', 'nevertheless', 'bottom', 'other', 'almost', 'well', 'hereupon', 'rather', 'became', 'elsewhere', 'who', 'nowhere', 'same', 'con', 'among', 'at', 'former', 'seemed', 'such', 'alone', 'sincere', 'up', 'than', 'except', 'via', 'can', 'everything', 'too', 'already', 'along', 'something', 'these', 'while', 'before', 'ourselves', 'he', 'i', 'anyone', 'our', 'thus', 'amoungst', 'its', 'but', 'keep', 'with', 'because', 'must', 'either', 'the', 'those', 'another', 'fill', 'first', 'it', 'move', 'hers', 'onto', 'thick', 'herein', 'thereupon', 'third', 'detail', 'latterly', 'always', 'between', 'over', 'had', 'ever', 'have', 'and', 'or', 'some', 'only', 'whereafter', 'twelve', 'when', 'eleven', 'hundred', 'whenever', 'also', 'below', 'in', 'if', 'see', 'whither', 'behind', 'again', 'less', 'much', 'others', 'hereby', 'his', 'whom', 'above', 'should', 'itself', 'themselves', 'although', 'around', 'sometime', 'until', 'whence', 'are', 'seem', 'mill', 'no', 'from', 'into', 'enough', 'part', 'she', 'us', 'least', 'will', 'anyway', 'describe', 'found', 'your', 'few', 'un', 'amongst', 'formerly', 'herself', 'however', 'has', 'of', 'within', 'down', 'five', 'cry', 'once', 'seeming', 'were', 'whole', 'co', 'couldnt', 'them', 'somewhere', 'nine', 'everywhere', 'him', 'neither', 'twenty', 'under', 'front', 'do', 'whereas', 'on', 'an', 'anything', 'afterwards', 'therein', 'yours', 'being', 'go', 'off', 'would', 'across', 'besides', 'very', 'wherein', 'empty', 'beyond', 'through', 'next', 'become', 'most', 'wherever', 'etc', 'against', 'cannot', 'sixty', 'otherwise', 'throughout', 'here', 'is', 'two', 'yet', 'without', 'namely', 'therefore', 'mostly', 'during', 'many', 'ltd', 'whoever'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "# two sentences with 21 words, forms i=2, j = 21 matrix\n",
    "text = ['' for i in range(2)]\n",
    "text[0]=\"SciPy provides tools for creating sparse matrices using multiple data structures\"\n",
    "text[1]=\"A dense matrix stored in a NumPy array can be converted into a sparse matrix\"\n",
    "print('''\n",
    "\n",
    "Here is the source text:\n",
    "{}\n",
    "'''.format(text))\n",
    "\n",
    "\n",
    "# Create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "#Tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "#summarize the vocabulary\n",
    "print('Below is the vocabulary based on the text above:')\n",
    "v = vectorizer.vocabulary_\n",
    "print(v)\n",
    "print('')\n",
    "print('Total vocabularies: {} '.format(len(v)))\n",
    "print('')\n",
    "print(\"token(word) 'sparse' has index: {} \".format(vectorizer.vocabulary_.get('sparse')))\n",
    "print()\n",
    "print(\"index 11 has token (word): '{}' \".format(vectorizer.get_feature_names()[11]))\n",
    "print()\n",
    "\n",
    "print(\"Let's count the the tokens\")\n",
    "vector = vectorizer.transform(text)\n",
    "print('''After transform() to count the tokens, the word count feature has the following shape: \n",
    "{}'''.format(vector.shape))\n",
    "\n",
    "print('''\n",
    "The word count feature type is: \n",
    "{}'''.format(type(vector)))\n",
    "\n",
    "print('''\n",
    "Word count feature presented as sparse matrix:\n",
    "{}'''.format(vector))\n",
    "\n",
    "va = vector.toarray()\n",
    "print('''\n",
    "Convert word count feature from sparse matrix to dense matrix(2 by 21):\n",
    "{}'''.format(va))\n",
    "\n",
    "sparsity = 1.0 - count_nonzero(va) / va.size\n",
    "print('The input array sparsity: {}'.format(sparsity))\n",
    "\n",
    "\n",
    "print('''\n",
    "The stop words list in skit learn library:\n",
    "{}\n",
    "'''.format(stop_words.ENGLISH_STOP_WORDS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction - TFIDF\n",
    "\n",
    "\n",
    "- Word counts are good starting point\n",
    "- Issue: some words will appear many times, and their large counts will not be very meaningful in the encoded matrix\n",
    "- An alternative way is to calculate word frequencies using TF-IDF\n",
    "    - Term Frequence (TF): summarizes how often a given word appears within a document\n",
    "    - Inverse Document Frequency (IDF): term weight function, and a measure of how much information the word provides\n",
    "    \n",
    "- Python libraries providing out of box TFIDF calcuation\n",
    "    - scikit learn\n",
    "    - Gensim \n",
    "    - PySpark ML lib\n",
    "    - nltk\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF Calculation (Term Frequency - Inverse Document Frequency):\n",
    "\n",
    "### $tfidf(t,d) = tf(t,d) * idf(t)$\n",
    " \n",
    "- n: the total number of documents in the document set \n",
    "\n",
    "\n",
    "-  df(t): * the number of documents in the document set that contain term t (smooth_idf = False):\n",
    "### $idf(t) = log \\frac{n}{df(t)} + 1$\n",
    "\n",
    "\n",
    "- df(t): * tfidf with smooth_idf = True *\n",
    "### $idf(t) = log \\frac{1+n}{1+df(t)} + 1$\n",
    "\n",
    "\n",
    "- Different flavored tfidf (standard textbook):\n",
    "### $idf(t) = log \\frac{n}{1+df(t)}$\n",
    "\n",
    "\n",
    "- The resulting tf-idf vectors are then ```normalized``` by Euclidean norm (a function that assigns a strictly positive length or size to each vector in a vector space)\n",
    "## $v_{norm} = \\frac{V}{|v|} = \\frac{[v_1, v_2, ..., v_n]}{\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0, 1], [2, 0, 0], [3, 0, 0], [4, 0, 0], [3, 2, 0], [3, 0, 2]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [\n",
    "[3, 0, 1],\n",
    "[2, 0, 0],\n",
    "[3, 0, 0],\n",
    "[4, 0, 0],\n",
    "[3, 2, 0],\n",
    "[3, 0, 2]\n",
    "]\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81940995, 0.        , 0.57320793],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.47330339, 0.88089948, 0.        ],\n",
       "       [0.58149261, 0.        , 0.81355169]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Steps of calcuation:\n",
    "\n",
    "\n",
    "\n",
    "- doc1 term1 calcuation (Smooth = false): \n",
    "    - n = 6 (6 documents)\n",
    "    - $df(t)_{term1} = 6$ (term1 appears in all 6 documents)\n",
    "    - $idf(t)_{term1} = log\\frac{n}{df(t)} + 1 = log(\\frac {6}{6}) + 1 = 1$\n",
    "    - $tfidf_{term1} = tf * idf = 3 * 1 = 3$ (term1 appears 3 times in document 1)\n",
    "\n",
    "\n",
    "\n",
    "- doc1 term2 calculation:\n",
    "    - n = 6\n",
    "    - $df(t)_{term2} = 1$ (term2 appears in document No 5)\n",
    "    - $idf(t)_{term2} = log\\frac{n}{df(t)} + 1 = log(\\frac {6}{1}) + 1 \\approx 2.792$\n",
    "    - $tfidf_{term2} = tf * idf \\approx 0 * 2.792 = 0$ (term1 appears 3 times in document 1)\n",
    "   \n",
    "   \n",
    "\n",
    "- doc1 term3 calculation:\n",
    "    - n = 6\n",
    "    - $df(t)_{term3} = 2$ (term3 appears in document No 1 and document No 6)\n",
    "    - $idf(t)_{term3} = log\\frac{n}{df(t)} + 1 = log(\\frac{6}{2}) + 1 \\approx 2.0986$\n",
    "    - $tfidf_{term3} = tf * idf = 1 * 2.0986 \\approx 2.0986$ (term1 appears 3 times in document 1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "- Use Euclidean norm, tfidf for first document:\n",
    "    - $\\frac{[3,  0,  2.0986]}{\\sqrt{3^2 + 0^2 + 2.0986^2}} = [0.819, 0, 0.573]$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85151335, 0.        , 0.52433293],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.55422893, 0.83236428, 0.        ],\n",
       "       [0.63035731, 0.        , 0.77630514]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = TfidfTransformer(smooth_idf=True)\n",
    "tfidf = transform.fit_transform(counts)\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Steps of calcuation using Smooth = True:\n",
    "\n",
    "\n",
    "\n",
    "- doc1 term1 calcuation (Smooth = True): \n",
    "    - n = 6 (6 documents)\n",
    "    - $df(t)_{term1} = 6$ (term1 appears in all 6 documents)\n",
    "    - $idf(t)_{term1} = log\\frac{n + 1}{df(t) + 1} + 1 = log(\\frac{7}{7}) + 1 = 0$\n",
    "    - $tfidf_{term1} = tf * idf = 3 * 1 = 3$ (term1 appears 3 times in document 1)\n",
    "\n",
    "\n",
    "\n",
    "- doc1 term2 calculation:\n",
    "    - n = 6\n",
    "    - $df(t)_{term2} = 1$ (term2 appears in document No 5)\n",
    "    - $idf(t)_{term2} = log\\frac{n + 1}{df(t) + 1} + 1 = log(\\frac{7}{2}) + 1 \\approx 2.2527$\n",
    "    - $tfidf_{term2} = tf * idf = 0 * 2.2527 = 0$ (term1 appears 3 times in document 1)\n",
    "   \n",
    "   \n",
    "\n",
    "- doc1 term3 calculation:\n",
    "    - n = 6\n",
    "    - $df(t)_{term3} = 2$ (term3 appears in document No 1 and document No 6)\n",
    "    - $idf(t)_{term3} = log\\frac{n + 1}{df(t) + 1} + 1 = log(\\frac{7}{3}) + 1 \\approx 1.8473$\n",
    "    - $tfidf_{term3} = tf * idf = 1 * 1.8473 \\approx 1.8473$ (term1 appears 3 times in document 1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "- Use Euclidean norm, tfidf for first document:\n",
    "    - $\\frac{[3, 0, 1.8473]}{\\sqrt{3^2 + 0^2 + 1.8473^2}} = [0.8515, 0, 0.5243]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer parameters in sklearn: \n",
      "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
      "         use_idf=True) \n",
      "\n",
      "\n",
      "In the previous word count example we got sparse matrix of 'vector' for two sentences in 'text'\n",
      "This sparse matrix will be used to Tfidf transformation \n",
      "\n",
      "\n",
      "Let's transform into Tfidf matrix\n",
      "\n",
      "\n",
      "Tfidf sparse matrix output:\n",
      "  (0, 20)\t0.3108525457240639\n",
      "  (0, 19)\t0.3108525457240639\n",
      "  (0, 18)\t0.3108525457240639\n",
      "  (0, 16)\t0.18359452107480756\n",
      "  (0, 15)\t0.3108525457240639\n",
      "  (0, 14)\t0.3108525457240639\n",
      "  (0, 12)\t0.3108525457240639\n",
      "  (0, 10)\t0.3108525457240639\n",
      "  (0, 7)\t0.3108525457240639\n",
      "  (0, 5)\t0.3108525457240639\n",
      "  (0, 4)\t0.3108525457240639\n",
      "  (1, 17)\t0.27370229648379657\n",
      "  (1, 16)\t0.16165298541458145\n",
      "  (1, 13)\t0.27370229648379657\n",
      "  (1, 11)\t0.5474045929675931\n",
      "  (1, 9)\t0.27370229648379657\n",
      "  (1, 8)\t0.27370229648379657\n",
      "  (1, 6)\t0.27370229648379657\n",
      "  (1, 3)\t0.27370229648379657\n",
      "  (1, 2)\t0.27370229648379657\n",
      "  (1, 1)\t0.27370229648379657\n",
      "  (1, 0)\t0.27370229648379657\n",
      "\n",
      "\n",
      "Tfidf sparse matrix shape:\n",
      "(2, 21)\n",
      "\n",
      "\n",
      "Convert tfidf into dense matrix:\n",
      "[[0.         0.         0.         0.         0.31085255 0.31085255\n",
      "  0.         0.31085255 0.         0.         0.31085255 0.\n",
      "  0.31085255 0.         0.31085255 0.31085255 0.18359452 0.\n",
      "  0.31085255 0.31085255 0.31085255]\n",
      " [0.2737023  0.2737023  0.2737023  0.2737023  0.         0.\n",
      "  0.2737023  0.         0.2737023  0.2737023  0.         0.54740459\n",
      "  0.         0.2737023  0.         0.         0.16165299 0.2737023\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "tfidf for token 'sparse' feature index: 16\n",
      "tfidf for token 'sparse' in document 1: 0.18359452107480756\n",
      "tfidf for token 'sparse' in document 2: 0.16165298541458145\n",
      "\n",
      "\n",
      "\n",
      "tfidf for token 'data' feature index: 5\n",
      "tfidf for token 'data' in document 1: 0.3108525457240639\n",
      "tfidf for token 'data' in document 2: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "print('''Transformer parameters in sklearn: \n",
    "{} '''.format(transformer))\n",
    "\n",
    "print('''\n",
    "\n",
    "In the previous word count example we got sparse matrix of 'vector' for two sentences in 'text'\n",
    "This sparse matrix will be used to Tfidf transformation ''')\n",
    "tfidf = transformer.fit(vector)\n",
    "\n",
    "print('''\n",
    "\n",
    "Let's transform into Tfidf matrix\n",
    "''')\n",
    "tfidf =transformer.transform(vector)\n",
    "\n",
    "print('''\n",
    "Tfidf sparse matrix output:\n",
    "{}\n",
    "'''.format(tfidf))\n",
    "\n",
    "print('''\n",
    "Tfidf sparse matrix shape:\n",
    "{}\n",
    "'''.format(tfidf.shape))\n",
    "\n",
    "print('''\n",
    "Convert tfidf into dense matrix:\n",
    "{}\n",
    "'''.format(tfidf.toarray()))\n",
    "\n",
    "idx = vectorizer.vocabulary_.get('sparse')\n",
    "tfidf_idx_1 = tfidf[0,idx]\n",
    "tfidf_idx_2 = tfidf[1,idx]\n",
    "print('''\n",
    "\n",
    "tfidf for token 'sparse' feature index: {}\n",
    "tfidf for token 'sparse' in document 1: {}\n",
    "tfidf for token 'sparse' in document 2: {}\n",
    "'''.format(idx, tfidf_idx_1, tfidf_idx_2))\n",
    "\n",
    "\n",
    "idx = vectorizer.vocabulary_.get('data')\n",
    "tfidf_idx_1 = tfidf[0,idx]\n",
    "tfidf_idx_2 = tfidf[1,idx]\n",
    "print('''\n",
    "\n",
    "tfidf for token 'data' feature index: {}\n",
    "tfidf for token 'data' in document 1: {}\n",
    "tfidf for token 'data' in document 2: {}\n",
    "'''.format(idx, tfidf_idx_1, tfidf_idx_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next Step: Build model Using Extracted Features\n",
    "\n",
    "i.e. Text Classification\n",
    "- LDA\n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References:\n",
    "- [working with text data] https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- [Feature Extraction] https://scikit-learn.org/stable/modules/feature_extraction.html#\n",
    "- [Feature Selection] https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection\n",
    "- [Sparse Matrix and Machine Learning] https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "- [Bag of Words Model] https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "- [spark ML lib Feature Extraction] https://spark.apache.org/docs/latest/mllib-feature-extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
